"""
Alerting System + API

Stores and manages alerts generated by the API degradation detection pipeline.
Provides persistence and retrieval of alerts with full context and explanations.
Includes REST API endpoints for alert retrieval.

This is the final component that makes the system real - without alert output,
the system effectively doesn't exist for users.
"""

from typing import Dict, Any, List, Optional
import os
import json
import time
import httpx
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

# Import helpers with fallbacks so `src` can be added to PYTHONPATH for tests
try:
    from .config import db_path as _config_db_path, LOG_LEVEL, ALERTS_FILE
    from .logging_config import configure_logging
    from .storage.alert_store import AlertStore as _AlertStoreImpl
except Exception:
    # Best-effort fallback: load modules directly from files adjacent to this
    # module so the package import style doesn't matter (useful for test scripts).
    import importlib.util
    from pathlib import Path

    base = Path(__file__).resolve().parent

    def _load_module_from(path: Path, name: str):
        spec = importlib.util.spec_from_file_location(name, str(path))
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)  # type: ignore
        return module

    cfg_mod = _load_module_from(base / 'config.py', 'local_config')
    _config_db_path = cfg_mod.db_path
    LOG_LEVEL = getattr(cfg_mod, 'LOG_LEVEL', 'INFO')
    ALERTS_FILE = getattr(cfg_mod, 'ALERTS_FILE', None)

    logging_mod = _load_module_from(base / 'logging_config.py', 'local_logging_config')
    configure_logging = logging_mod.configure_logging

    storage_mod = _load_module_from(base / 'storage' / 'alert_store.py', 'local_alert_store')
    _AlertStoreImpl = storage_mod.AlertStore

# initialize structured logging
configure_logging(LOG_LEVEL)

# Legacy imports for backward compatibility
SLACK_WEBHOOK = os.environ.get('SLACK_WEBHOOK_URL')


class AlertStore:
    """Backward-compatible wrapper exposing the original `AlertStore` API.

    Accepts an optional `db_path`. If omitted the path from configuration
    will be used.
    """

    def __init__(self, db_path: Optional[str] = None):
        db_path = db_path or _config_db_path()
        self._impl = _AlertStoreImpl(db_path)

    def store_alert(self, alert: Dict[str, Any]) -> str:
        return self._impl.store_alert(alert)

    def get_alert(self, alert_id: str) -> Optional[Dict[str, Any]]:
        return self._impl.get_alert(alert_id)

    def get_all_alerts(self, limit: int = 100, status: Optional[str] = None) -> List[Dict[str, Any]]:
        return self._impl.get_all_alerts(limit=limit, status=status)

    def update_alert_status(self, alert_id: str, status: str) -> bool:
        return self._impl.update_alert_status(alert_id, status)


# Module-level singleton accessor for the web app handlers
_global_store: Optional[AlertStore] = None


def get_alert_store() -> AlertStore:
    global _global_store
    if _global_store is None:
        _global_store = AlertStore()
    return _global_store


def store_alerts(explained_alerts: List[Dict[str, Any]]) -> List[str]:
    """
    Store multiple explained alerts and return their IDs.

    Each alert must have been processed by the explainer.
    """
    store = get_alert_store()
    alert_ids = []

    for alert in explained_alerts:
        # Ensure timestamp is set
        if 'timestamp' not in alert:
            alert['timestamp'] = datetime.now(timezone.utc).isoformat()

        alert_id = store.store_alert(alert)
        alert_ids.append(alert_id)

    return alert_ids


# Legacy functions for backward compatibility
def classify_severity(anomalies: List[Dict[str, Any]]) -> str:
    """Legacy severity classification."""
    if not anomalies:
        return 'INFO'

    num_signals = len(anomalies)
    has_high_error = any(a.get('metric') == 'error_rate' and a.get('current_value', 0) > 0.1 for a in anomalies)
    has_high_latency = any(a.get('metric') in ['avg_latency', 'p95_latency'] and abs(a.get('deviation', 0)) > 3.0 for a in anomalies)
    has_critical_severity = any(a.get('severity') == 'CRITICAL' for a in anomalies)

    if has_high_error or has_high_latency or num_signals > 3 or has_critical_severity:
        return 'CRITICAL'
    elif any(a.get('metric') in ['avg_latency', 'p95_latency'] and abs(a.get('deviation', 0)) > 2.0 for a in anomalies) or num_signals > 2:
        return 'WARN'
    else:
        return 'INFO'


def send_console(alert: Dict[str, Any]):
    """Legacy console output."""
    ts = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
    print('--- ALERT ---')
    print(f"time: {ts}")
    print(json.dumps(alert, indent=2))


def send_slack(alert: Dict[str, Any]):
    """Legacy Slack notification."""
    if not SLACK_WEBHOOK:
        return False
    text = f"*[{alert.get('severity','WARN')}]* {alert.get('endpoint')} - {alert.get('explanation')}"
    payload = {"text": text}
    try:
        r = httpx.post(SLACK_WEBHOOK, json=payload, timeout=5.0)
        return r.status_code == 200
    except Exception:
        return False


def alert(alert_obj: Dict[str, Any]):
    """Legacy alert function - now also stores in database."""
    # Classify severity if not set
    if 'severity' not in alert_obj:
        anomalies = alert_obj.get('anomalies', [])
        alert_obj['severity'] = classify_severity(anomalies)

    # Store in database if it has the required fields
    if all(key in alert_obj for key in ['endpoint', 'severity', 'anomalous_metrics', 'explanation']):
        try:
            store_alerts([alert_obj])
        except Exception:
            pass  # Fall back to legacy behavior

    # Legacy behavior
    send_console(alert_obj)

    # persist alert for offline analysis (legacy)
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
        with open(ALERTS_FILE, 'a', encoding='utf-8') as fh:
            fh.write(json.dumps(alert_obj) + '\n')
    except Exception:
        pass

    # try slack
    ok = send_slack(alert_obj)
    return ok


# FastAPI Application
print("DEBUG: Creating FastAPI app...")
app = FastAPI(
    title="API Degradation Detection - Alerting API",
    description="REST API for retrieving alerts from the API degradation detection system",
    version="1.0.0"
)
print("DEBUG: FastAPI app created")

# Add CORS middleware
print("DEBUG: Adding CORS middleware...")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
print("DEBUG: CORS middleware added")


# Lifecycle event hooks for better diagnostics
@app.on_event("startup")
async def _on_startup():
    print("LIFECYCLE: startup event fired")
    try:
        # Initialize alert store to ensure DB is ready at startup
        get_alert_store()
        print("LIFECYCLE: AlertStore initialized")
    except Exception as e:
        print(f"LIFECYCLE: AlertStore initialization failed: {e}")


@app.on_event("shutdown")
async def _on_shutdown():
    print("LIFECYCLE: shutdown event fired")


@app.get("/")
async def root():
    """Root endpoint - redirects to API documentation."""
    return {
        "message": "API Degradation Detection - Alerting API",
        "version": "1.0.0",
        "docs": "/docs",
        "endpoints": {
            "GET /": "API information and endpoint list",
            "GET /alerts": "Get all alerts",
            "GET /alerts/{id}": "Get specific alert",
            "PUT /alerts/{id}/status": "Update alert status",
            "GET /metrics/{endpoint}": "Get metrics for endpoint",
            "GET /health": "Health check"
        }
    }


@app.get("/alerts")
async def get_alerts(limit: int = 100, status: str = None):
    """
    Get all alerts.

    - **limit**: Maximum number of alerts to return (default: 100)
    - **status**: Filter by status (active, acknowledged, resolved)
    """
    try:
        print("DEBUG: Getting alert store...")
        store = get_alert_store()
        print("DEBUG: Fetching alerts...")
        alerts = store.get_all_alerts(limit=limit, status=status)
        print(f"DEBUG: Retrieved {len(alerts)} alerts")

        # Format alerts for API response (include required fields)
        formatted_alerts = []
        for alert in alerts:
            try:
                formatted_alert = {
                    'id': alert['id'],
                    'endpoint': alert['endpoint'],
                    'severity': alert['severity'],
                    'metrics_involved': alert['anomalous_metrics'],
                    'explanation': alert['explanation'],
                    'timestamp': alert['timestamp'],
                    'status': alert['status'],
                    'anomaly_count': alert['anomaly_count'],
                    'insights': alert['insights'],
                    'recommendations': alert['recommendations']
                }
                formatted_alerts.append(formatted_alert)
            except Exception as e:
                print(f"DEBUG: Error formatting alert {alert.get('id', 'unknown')}: {e}")
                continue

        print(f"DEBUG: Formatted {len(formatted_alerts)} alerts")
        return {"alerts": formatted_alerts, "count": len(formatted_alerts)}

    except Exception as e:
        print(f"DEBUG: Error in get_alerts: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to retrieve alerts: {str(e)}")


@app.get("/alerts/{alert_id}")
async def get_alert(alert_id: str):
    """
    Get a specific alert by ID.

    - **alert_id**: The unique identifier of the alert
    """
    try:
        store = get_alert_store()
        alert = store.get_alert(alert_id)

        if not alert:
            raise HTTPException(status_code=404, detail="Alert not found")

        # Format alert for API response
        formatted_alert = {
            'id': alert['id'],
            'endpoint': alert['endpoint'],
            'severity': alert['severity'],
            'metrics_involved': alert['anomalous_metrics'],
            'explanation': alert['explanation'],
            'timestamp': alert['timestamp'],
            'status': alert['status'],
            'anomaly_count': alert['anomaly_count'],
            'insights': alert['insights'],
            'recommendations': alert['recommendations'],
            'window': alert['window'],
            'avg_deviation': alert['avg_deviation'],
            'max_deviation': alert['max_deviation'],
            'created_at': alert['created_at']
        }

        return formatted_alert

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve alert: {str(e)}")


@app.put("/alerts/{alert_id}/status")
async def update_alert_status(alert_id: str, status: str):
    """
    Update the status of an alert.

    - **alert_id**: The unique identifier of the alert
    - **status**: New status (active, acknowledged, resolved)
    """
    if status not in ['active', 'acknowledged', 'resolved']:
        raise HTTPException(status_code=400, detail="Invalid status. Must be: active, acknowledged, or resolved")

    try:
        store = get_alert_store()
        success = store.update_alert_status(alert_id, status)

        if not success:
            raise HTTPException(status_code=404, detail="Alert not found")

        return {"message": f"Alert {alert_id} status updated to {status}"}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to update alert status: {str(e)}")


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "healthy", "service": "API Degradation Detection - Alerting API"}


@app.get("/metrics/{endpoint}")
async def get_metrics(endpoint: str, window: str = "15m"):
    """
    Get metrics for a specific endpoint.

    This is a mock endpoint that returns sample metrics data for demonstration.
    In a real system, this would query stored aggregated metrics.
    """
    try:
        # For now, return mock metrics data that shows degradation over time
        import random
        from datetime import datetime, timedelta

        # Generate sample metrics over the last 15 minutes
        metrics = []
        base_time = datetime.utcnow()

        for i in range(15):
            timestamp = base_time - timedelta(minutes=14-i)

            # Simulate degradation starting around minute 8
            if i < 8:
                # Normal performance
                avg_latency = 120 + random.randint(-10, 10)
                p95_latency = avg_latency * 1.5
                error_rate = 0.02 + random.uniform(-0.01, 0.01)
                request_volume = 100 + random.randint(-20, 20)
            else:
                # Degraded performance
                degradation_factor = (i - 7) / 8  # 0 to 1
                avg_latency = 120 + (800 - 120) * degradation_factor + random.randint(-20, 20)
                p95_latency = avg_latency * 1.8
                error_rate = 0.02 + (0.18 - 0.02) * degradation_factor + random.uniform(-0.02, 0.02)
                request_volume = 100 + random.randint(-30, 30)

            metrics.append({
                "window_end": timestamp.isoformat() + "Z",
                "avg_latency": round(avg_latency, 1),
                "p95_latency": round(p95_latency, 1),
                "error_rate": round(error_rate, 3),
                "request_volume": request_volume,
                "endpoint": endpoint
            })

        return metrics

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to retrieve metrics: {str(e)}")


if __name__ == '__main__':
    print('Alerting system loaded - ready to store and serve alerts!')
    print('SLACK_WEBHOOK_URL set?', bool(SLACK_WEBHOOK))
